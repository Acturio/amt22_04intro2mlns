[["index.html", "AMAT- Ciencia de Datos y Machine Learning Aprendizaje NO supervisado BIENVENIDA Objetivo Instructores Estructura del curso Duración y evaluación del curso Recursos y dinámica de clase", " AMAT- Ciencia de Datos y Machine Learning Aprendizaje NO supervisado Karina Lizette Gamboa Puente Oscar Arturo Bringas López BIENVENIDA Objetivo Brindar al participante un amplio conocimiento de los requisitos, alcances y toma de decisión alrededor de un proyecto de Ciencia de Datos. Aprenderá a través del conjunto de librerías más novedoso en R a crear, ajustar y afinar los mejores parámetros para la correcta implementación de diferentes tipos de modelos de aprendizaje de máquina estadístico no supervisado (Not Supervised Statistical Machine Learning) para la creación de índices, segmentación y visualización de datos multivariados. De entre los distintos modelos creados, sabrá identificar cuál modelo ofrece el mejor ajuste de acuerdo con el objetivo particular de cada problema a resolver. Se asume que el alumno tiene conocimientos generales de matemáticas, geometría analítica o álgebra lineal y de programación básica en R. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos por el ITAM. Se especializa en modelos predictivos y de clasificación de machine learning aplicado a seguros, marketing, deportes y movilidad internacional. Es jefe de departamento en Investigación Aplicada y Opinión de la UNAM, donde realiza estudios estadísticos de impacto social. Ha sido consultor Sinior Data Scientist para empresas y organizaciones como GNP, El Universal, UNAM, Sinnia, Geekend, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de Ciencia de datos y Machine Learning en AMAT y se desempeña como consultor independiente en diferentes proyectos contribuyendo a empresas en temas de análisis estadístico y ciencia de datos con machine learning. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria, egresada de la Facultad de Ciencias, UNAM, candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Estructura del curso Alcances del curso Al finalizar el módulo, el participante sabrá plantear un proyecto de ciencias de datos, desde sus requerimientos hasta sus alcances. Sabrá crear flujos de trabajo limpios y ordenados para crear poderosos modelos no supervisados de Machine Learning. Requisitos: Computadora con al menos 4Gb Ram. Instalación de R con al menos versión 4.1.0 Instalación de Rstudio con al menos versión 1.4 Data Science &amp; Machine Learning (Aprendizaje Supervisado I) Temario: 1.- Introducción a Ciencia de Datos (4 HRS) ¿Qué es Ciencia de Datos? Objetivo de la ciencia de datos ¿Qué se requiere para hacer ciencia de datos? Tipos de problemas que se pueden resolver Tipos de algoritmos y aprendizaje Ciclo de vida de un proyecto de Ciencia de Datos 2. Análisis de componentes principales (4 HRS) Planteamiento Construcción de componentes Reducción de dimensión Visualización Aplicaciones 3. Cluster Jerárquico (4 HRS) Liga simple Liga compleja Liga promedio Dendogramas &amp; Heatmap 4. Cluster no Jerárquico (6 HRS) K-means K-medoides Clara DBSCAN 5. Clusterización y visualización geoespacial (6 HRS) Lectura de datos espaciales Transformación de datos espaciales Clustering espacial Visualización espacial Duración y evaluación del curso El programa tiene una duración de 24 hrs. Las clases serán impartidas los días sábados, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. Recursos y dinámica de clase En esta clase estaremos usando: R da click aquí si aún no lo descargas RStudio da click aquí también Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar Grupo de WhatsApp El chismecito está aquí Google Drive Notas de clase Revisame si quieres aprender Finalmente, se dejarán ejercicios que serán clave para el éxito del aprendizaje de los capítulos, por lo que se trabajará en equipo para lograr adquirir el mayor aprendizaje. "],["introducción-a-ciencia-de-datos.html", "Capítulo 1 Introducción a Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? 1.2 Objetivos 1.3 Requisitos 1.4 Aplicaciones 1.5 Tipos de algoritmos 1.6 Ciclo de un proyecto", " Capítulo 1 Introducción a Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? Definiendo conceptos: Estadística Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. Se utilizan números (media, mediana, moda, mínimo, máximo, etc) para analizar datos y llegar a conclusiones de acuerdo a ellos. La estadística inferencial argumenta o infiere sus resultados a partir de las muestras de una población. Se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. La estadística predictiva busca estimar valores y escenarios futuros más probables de ocurrir a partir de referencias históricas previas. Se suelen ocupar como apoyo características y factores áltamente asociados al fenómeno que se desea predecir. Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. BI esta enfocado en analizar la historia pasada para tomar decisiones hacia el futuro. ¿Qué características tiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: Machine learning –aprendizaje de máquina– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imágenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es común que se confunda los conceptos de Big Data y Big Compute, como se mencionó, Big Data se refiere al procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 1.2 Objetivos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 1.3 Requisitos Background científico: Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría analítica, programación, conocimientos computacionales… etc Datos relevantes y suficientes: Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Software: Existen distintos lenguajes de programación para realizar ciencia de datos: 1.4 Aplicaciones Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a continuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. Otros ejemplos Upselling y Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Optimización de precios Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) Detección de transacciones ilícitas Detección de servicios fraudulentos 1.5 Tipos de algoritmos Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.5.1 Aprendizaje supervisado En el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. Estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien.” Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 1.5.2 Aprendizaje no supervisado En el aprendizaje no supervisado, carecemos de etiquetas. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir ¿qué es qué? por nosotros mismos. Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características Algunos ejemplos son: Agrupar clientes según las visitas y comportamiento en la web. Reducir la complejidad de un problema. Selección de variables para un modelo diferente. Reducción de dimensionalidad. Determinar los distintos patrones climáticos de una región Agrupar artículos o noticias por temas. Descubrir zonas con elevadas tasas de criminalidad. Regionalizaciones sociales, económicas, políticas o por algún otro interés. Grupos de personalidades según las respuestas de una encuesta. Grupos de productos por composición (vinos o metales) 1.5.3 Aprendizaje por refuerzo Su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Optimización de campañas de marketing - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN Ejemplo: Mario Bros 1.6 Ciclo de un proyecto Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos.” Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. "],["scoping.html", "Capítulo 2 Scoping", " Capítulo 2 Scoping El scoping es uno de los pasos más importante en los proyectos de ciencia de datos, es ideal realizarlo con ayuda del cliente, tiene como objetivo definir el alcance del proyecto, definir los objetivos, conocer las acciones que se llevaran acabo, conocer si los datos son relevantes y suficientes, proponer soluciones analíticas, entre otros puntos que se tocaran a continuación. El proceso a seguir es el siguiente: Definir objetivo(s) Considerado el paso más importante del proceso, los stakeholders iniciaran con un planteamiento del problema de manera muy general, nuestra responsabilidad será ir aterrizando ideas y definir el problema de manera más concreta, esta parte del scoping puede ocurrir en distintas iteraciones. Necesitamos hacer que el objetivo sea concreto, medible y optimizable. Cuando se van refinando objetivos, es común que se vaya priorizando por lo que tendremos tradeoffs que irán ligados a las acciones y al contexto del negocio. ¿Qué acciones o intervenciones existen que serán mejoradas a través de este proyecto? Debemos definir acciones concretas, si esto no ocurre es muy probable que la solución no sea implementada por lo que el proyecto no tendrá uso y no se estará haciendo ciencia de datos. La implementación del proyecto debería ayudar a tener mejor información para llevar acabo estas acciones, es decir, el proyecto mejorará la toma de decisiones basadas en la evidencia de los datos. Hacer una lista con las acciones ayuda a que el proyecto sea accionable, es posible que estas acciones no existan aún en la organización, por lo que el proyecto puede ayudar a generar nuevas acciones. Es muy común que la acción definida por el stakeholder sea de muy alto nivel, en ese caso podemos tomar 2 caminos en el scoping: Proponer en el scoping que el proyecto informe a esa acción general. Generar a partir de esa acción general acciones más pequeñas. ¿Qué datos tenemos y cuáles necesitamos? Primero observemos que no se había hablado de los datos hasta este punto, lo anterior porque debemos primero pensar en el problema, entenderlo y luego ver con qué datos contamos para resolverlo. Si hacemos esto primero seguramente acabaremos desarrollando productos de datos “muertos” y no accionables. En este paso se le dará uso al Data Maturity Framework, queremos conocer cómo se guardan los datos, con qué frecuencia, en qué formato, en qué estructura, qué granularidad tiene, desde cuándo tenemos historia de estos datos, si existe un sesgo en su recolección, con qué frecuencia recolectan nueva información, sobrescribe la ya existente? Uno de los objetivos consiste en identificar si la granularidad, frecuencia y horizonte de tiempo en los datos corresponde a la granularidad, frecuencia y horizonte de tiempo de las acciones. Otro punto importante es saber si los datos con los que se cuenta son relevantes y suficientes para desarrollar el proyecto, se pueden considerar fuentes de datos externa. ¿Cuál es el análisis que necesitamos hacer? En esta sección del scoping queremos definir qué tipo de análisis necesitamos hacer con los datos con los que contamos para cumplir con los objetivos definidos y generar las acciones identificadas. El análisis puede incluir métodos y herramientas de diferentes disciplinas: ciencias computacionales, ciencia de datos, machine learning, estadística, ciencias sociales. Existen distintos tipos de análisis, los 4 más comunes son: Descripción: Centrado en entender eventos y comportamientos del pasado. Aunque puede confundirse con business intelligence, debido a que ya definimos objetivos y acciones vamos a desarrollar un producto de datos. Para este tipo de análisis podemos ocupar métodos de aprendizaje no supervisado: clustering. Detección: Más concentrado en los eventos que están sucediendo. Detección de anomalías. Predicción: Concentrado en el futuro, prediciendo futuros eventos o comportamientos. Cambio en comportamiento: Concentrado en entender las causas de cambios en comportamientos de personas eventos, organizaciones, vecindarios, etc. En esta fase tenemos que responder las siguientes preguntas: ¿Qué tipo de análisis necesitaremos? Puede ser más de uno. ¿Cómo vamos a validar el análisis? ¿Qué validaciones se pueden hacer con los datos existentes? ¿Cómo podemos diseñar una prueba en campo para validar el análisis antes de que pongamos el producto en producción? Identificar qué acciones se cubren con cada análisis, debemos tener todas las acciones cubiertas. "],["análisis-de-componentes-principales.html", "Capítulo 3 Análisis de Componentes Principales 3.1 Medidas de dispersión 3.2 Construcción matemática 3.3 Implementación en R 3.4 Reducción de dimensión 3.5 Representación gráfica 3.6 Predicciones 3.7 Ejercicios", " Capítulo 3 Análisis de Componentes Principales El análisis PCA (por sus siglas en inglés) es una técnica de reducción de dimensión útil tanto para el proceso de análisis exploratorio, el inferencial y predictivo. Es una técnica ampliamente usada en muchos estudios, pues permite sintetizar la información relevante y desechar aquello que no aporta tanto. Es particularmente útil en el caso de conjuntos de datos “amplios” en donde las variables están correlacionadas entre sí y donde se tienen muchas variables para cada observación. En los conjuntos de datos donde hay muchas variables presentes, no es fácil trazar los datos en su formato original, lo que dificulta tener una idea de las tendencias presentes en ellos. PCA permite ver la estructura general de los datos, identificando qué observaciones son similares entre sí y cuáles son diferentes. Esto puede permitirnos identificar grupos de muestras que son similares y determinar qué variables hacen a un grupo diferente de otro. 3.1 Medidas de dispersión Las medidas de dispersión tratan, a través del cálculo de diferentes fórmulas, de arrojar un valor numérico que ofrezca información sobre el grado de variabilidad de una característica En otras palabras, las medidas de dispersión son números que indican si una variable se mueve mucho, poco, más o menos que otra. La razón de ser de este tipo de medidas es conocer de manera resumida una característica de la variable estudiada. En este sentido, deben acompañar a las medidas de tendencia central. Juntas, ofrecen información de un sólo vistazo que luego podremos utilizar para comparar y, si fuera preciso, tomar decisiones. Dos de las medidas principales son la varianza también denotada como \\(\\sigma^2\\) y la desviación estándar \\(\\sigma\\). La varianza es una medida de dispersión que representa la variabilidad de una serie de datos respecto a su media y la desviación estándar es la separación que existe entre un valor cualquiera de la serie y la media. Ejemplo: Tú y tus amigos han medido las alturas de sus perros (en milímetros): Las alturas (hasta el lomo de cada perro) son: 600mm, 470mm, 170mm, 430mm y 300mm. Si calculamos la media, sería: \\[ Media = \\frac{1}{n}\\sum_{i=1}^{n}{x_i} = \\frac{600 + 470 + 170 + 430 + 300}{5} = 394 \\] así que la altura media es 394 mm. Si se dibuja esto en el gráfico: Si ahora se calcula la diferencia de cada altura con la media: Para calcular la Varianza, se toma cada diferencia, se eleva al cuadrado y se calcula la media: \\[ Varianza = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\frac{206^2 + 76^2 + (−224)^2 + 36^2 + (−94)^2}{5} = 21,704\\] Por otro lado, la desviación estándar se calcula como la raíz cuadrada de la varianza, entonces: \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{21704} = \\pm 147 \\] Si se dibuja esta distancia, ahora se pueden ver qué alturas están dentro de una desviación estándar (147mm) de la media: Por lo tanto, usando la desviación estándar tenemos una manera “estándar” de saber qué es normal, o extra grande o extra pequeño. Los Rottweilers son perros grandes. Y los Dachsunds (perro salchicha) son un poco pequeños, ¿cierto? La tercera medida que se revisará es el coeficiente de correlación lineal (\\(\\rho\\)) que es también solo llamada “correlación” es una medida de regresión que pretende cuantificar el grado de variación conjunta entre dos variables. Dicha medida puede encontrarse entre -1 y 1, donde se pueden tener las siguientes interpretaciones dependiendo de su valor: En pocas palabras, mide qué tanto varía una característica en la medida en que varía otra. La fórmula para calcular el grado de correlación es: \\[Cor(x, y) = \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum(x_i-\\bar{x})^2}\\sqrt{\\sum(y_i-\\bar{y})^2}}\\] 3.2 Construcción matemática Sea \\(X\\) una matriz de \\(n\\) renglones y \\(p\\) columnas, se denota por \\(X_i\\) a la i-ésima columna que representa una característica del conjunto en su totalidad… Se desean crear nuevas variables llamadas Componentes Principales, las cuales son creadas como combinación lineal (suma ponderada) de las variables originales, por lo que cada una de las variables nuevas contiene parcialmente información de todas las variables originales. \\[Z_1 = a_{11}X_1 +a_{12}X_2 + ... + a_{1p}X_p\\] \\[Z_2 = a_{21}X_1 +a_{22}X_2 + ... + a_{2p}X_p\\] \\[...\\] \\[Z_p = a_{p1}X_1 +a_{p2}X_2 + ... + a_{pp}X_p\\] Donde: \\(Z_i\\) es la iésima componente nueva creada como combinación de las características originales \\(X_1, X_2, ... X_p\\) son las columnas (variables originales) \\(a_{ij}\\) es el peso o aportación de cada columna j a la nueva componente i. Se desea que la primer componente principal capture la mayor varianza posible de todo el conjunto de datos. \\[\\forall i \\in 2,...,p \\quad Var(Z_1)&gt;Var(Z_i)\\] La segunda componente principal deberá SER INDEPENDIENTE de la primera y deberá abarcar la mayor varianza posible del restante. Esta condición se debe cumplir para toda componente i, de tal forma que las nuevas componentes creadas son independientes entre sí y acumulan la mayor proporción de varianza en las primeras de ellas, dejando la mínima proporción de varianza a las últimas componentes. \\[Z_1 \\perp\\!\\!\\!\\perp Z_2 \\quad \\&amp; \\quad Var(Z_1)&gt;Var(Z_2)&gt;Var(Z_i)\\] El punto anterior permite desechar unas cuantas componentes (las últimas) sin perder mucha varianza. ¡¡ RECORDAR !! A través de CPA se logra retener la mayor cantidad de varianza útil pero usando menos componentes que el número de variables originales. Para que este proceso sea efectivo, debe existir ALTA correlación entre las variables originales. Cuando muchas variables se correlacionan entre sí, todas contribuirán fuertemente al mismo componente principal. Cada componente principal suma un cierto porcentaje de la variación total en el conjunto de datos. Cuando sus variables iniciales estén fuertemente correlacionadas entre sí y podrá aproximar la mayor parte de la complejidad de su conjunto de datos con solo unos pocos componentes principales. Agregar componentes adicionales hace que la estimación del conjunto de datos total sea más precisa, pero también más difícil de manejar. 3.2.1 Eigenvalores y eigenvectores Estimar la ponderación adecuada que debe tener cada una de características para crear las nuevas componentes es un paso crucial en este análisis. Se puede demostrar bajo una rigurosa metodología matemática que la solución que permite obtener resultados óptimos se logra cuando: \\[Xv=\\lambda v\\] Donde: \\(X\\) es la matriz de correlación calculada a partir de los datos originales \\(v\\) es el vector con los pesos de cada columna (eigenvector). \\(\\lambda\\) corresponde a la varianza de cada nueva componente (eigenvalor). Este resultado corresponde al cálculo de los eigenvectores \\(v\\) (vectores propios) y eigenvalores \\(\\lambda\\) (valores propios) de una matriz de datos. Los vectores propios y los valores propios vienen en pares: cada vector propio tiene un valor propio correspondiente. Los vectores propios son la ponderación que permite crear la combinación lineal de las variables para conformar cada componente principal, mientras que el valor propio es la varianza asociada a cada componente principal. El valor propio de una componente es la varianza de este. La suma acumulada de los primeros \\(j\\) eigenvalores representa la varianza acumulada de las primeras \\(j\\) componentes principales El número de valores propios y vectores propios que existe es igual al número de dimensiones que tiene el conjunto de datos. Para una mayor explicación matemática, este libro ofrece un amplio capítulo dedicado exclusivamente al análisis de componentes principales 3.3 Implementación en R Para ejemplificar el uso de CPA, usaremos los datos de CONAPO para replicar el índice de marginación social, el cual pretende dar una medida de pobreza por regiones, las cuales pueden ser entidades, municipios, localidades, agebs o incluso manzanas*. Existen MUUUCHAS librerías que facilitan el análisis de componentes principales. En este blog se puede encontrar la diferencia en su implementación. Todas ofrecen resultados útiles y confiables. library(sf) library(magrittr) library(tidymodels) indice_marg &lt;- st_read(&#39;data/IMEF_2010.dbf&#39;, quiet = TRUE) glimpse(indice_marg) ## Rows: 32 ## Columns: 16 ## $ CVE_ENT &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;1… ## $ AÑO &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20… ## $ POB_TOT &lt;int&gt; 1184996, 3155070, 637026, 822441, 2748391, 650555, 4796580, 34… ## $ ANALF &lt;dbl&gt; 3.274040, 2.600783, 3.234464, 8.370643, 2.645050, 5.157943, 17… ## $ SPRIM &lt;dbl&gt; 14.754823, 12.987567, 14.273833, 22.541207, 12.168029, 18.4761… ## $ OVSDE &lt;dbl&gt; 1.0649743, 0.4322072, 0.9436751, 6.4196750, 1.0916308, 0.68577… ## $ OVSEE &lt;dbl&gt; 0.62347891, 0.94517891, 2.84464884, 2.59080046, 0.53707721, 0.… ## $ OVSAE &lt;dbl&gt; 0.9854257, 3.5616214, 7.0865085, 9.7378176, 1.3908497, 1.17060… ## $ VHAC &lt;dbl&gt; 30.33066, 29.05839, 31.73806, 45.96720, 30.26891, 31.32052, 53… ## $ OVPT &lt;dbl&gt; 1.761813, 3.398537, 5.814081, 4.500699, 1.423701, 4.691477, 15… ## $ PL_5000 &lt;dbl&gt; 25.1626166, 10.3491523, 15.6188287, 30.8755279, 12.1486353, 14… ## $ PO2SM &lt;dbl&gt; 33.64880, 21.86970, 23.29986, 45.51076, 30.04270, 32.04402, 69… ## $ IM &lt;dbl&gt; -0.91086057, -1.14014880, -0.68128749, 0.43357139, -1.14000448… ## $ GM &lt;chr&gt; &quot;Bajo&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Alto&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Muy a… ## $ LUGAR &lt;int&gt; 28, 30, 23, 10, 29, 26, 2, 21, 32, 15, 14, 1, 6, 27, 22, 8, 19… ## $ NOM_ENT &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, &quot;C… indice_marg %&gt;% dplyr::count(GM, sort = TRUE) ## GM n ## 1 Medio 9 ## 2 Alto 8 ## 3 Bajo 8 ## 4 Muy bajo 4 ## 5 Muy alto 3 pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM, num_comp=9, res=&quot;res&quot;) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 13 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 PC5 PC6 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguasc… -0.911 -2.34 -0.227 0.372 0.492 0.264 0.0764 ## 2 02 Muy b… Baja C… -1.14 -2.93 0.595 -0.0597 -0.492 0.291 -0.0508 ## 3 03 Bajo Baja C… -0.681 -1.75 1.37 -0.683 -0.400 -0.304 0.160 ## 4 04 Alto Campec… 0.434 1.12 -0.819 -0.151 -0.271 -0.929 0.178 ## 5 05 Muy b… Coahui… -1.14 -2.93 -0.144 0.157 -0.133 0.0419 -0.0786 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 0.320 -0.729 ## 7 07 Muy a… Chiapas 2.32 5.96 0.132 1.36 -0.0122 -0.673 -0.471 ## 8 08 Bajo Chihua… -0.520 -1.34 1.05 -1.27 0.633 -0.646 -0.387 ## 9 09 Muy b… Distri… -1.48 -3.81 0.110 0.159 -0.453 0.205 -0.355 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 -0.448 0.146 ## # … with 22 more rows, and 3 more variables: PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;, PC9 &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Veamos los pasos de esta receta: Primero, debemos decirle a la receta qué datos se usan para predecir la variable de respuesta. Se actualiza el rol de las variables nombre de entidad y grado de marginación con la función NOM_ENT, ya que es una variable que queremos mantener por conveniencia como identificador de filas, pero no son un predictor ni variable de respuesta. Necesitamos centrar y escalar los predictores numéricos, porque estamos a punto de implementar PCA. Finalmente, usamos step_pca() para realizar el análisis de componentes principales. La función prep() es la que realiza toda la preparación de la receta. Una vez que hayamos hecho eso, podremos explorar los resultados del PCA. Comencemos por ver cómo resultó el PCA. Podemos ordenar los resultados mediante la función tidy(), incluido el paso de PCA, que es el segundo paso. Luego hagamos una visualización para ver cómo se ven los componentes. A continuación se muestran la desviación estándar, porcentaje de varianza y porcentaje de varianza acumulada que aporta cada componente principal. summary(pca_recipe$steps[[2]]$res) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.572 0.82085 0.7920 0.64640 0.52101 0.44069 0.31797 ## Proportion of Variance 0.735 0.07487 0.0697 0.04643 0.03016 0.02158 0.01123 ## Cumulative Proportion 0.735 0.80990 0.8796 0.92603 0.95619 0.97777 0.98900 ## PC8 PC9 ## Standard deviation 0.25660 0.18201 ## Proportion of Variance 0.00732 0.00368 ## Cumulative Proportion 0.99632 1.00000 Podemos observar que en la primera componente principal, las \\(9\\) variables que utilizó el Consejo Nacional de Población para obtener el Índice de Marginación 2010 aportan de manera positiva en el primer componente principal. library(tidytext) tidied_pca &lt;- tidy(pca_recipe, 2) tidied_pca %&gt;% filter(component %in% paste0(&quot;PC&quot;, 1:4)) %&gt;% group_by(component) %&gt;% top_n(9, abs(value)) %&gt;% ungroup() %&gt;% mutate(terms = reorder_within(terms, abs(value), component)) %&gt;% ggplot(aes(abs(value), terms, fill = value &gt; 0)) + geom_col() + facet_wrap(~component, scales = &quot;free_y&quot;) + scale_y_reordered() + labs( x = &quot;Absolute value of contribution&quot;, y = NULL, fill = &quot;Positive?&quot; )+ theme_minimal() Notamos que las \\(9\\) variables aportan entre el \\(25\\%\\) y el \\(35\\%\\) a la primera componente principal. 3.4 Reducción de dimensión Existe en la literatura basta información sobre el número de componentes a retener en un análisis de PCA. El siguiente gráfico lleva por nombre gráfico de codo y muestra el porcentaje de varianza explicado por cada componente principal. library(factoextra) library(FactoMineR) res.pca &lt;- indice_marg %&gt;% select(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% as.data.frame() %&gt;% set_rownames(indice_marg$NOM_ENT) %&gt;% FactoMineR::PCA(graph=FALSE) fviz_eig(res.pca, addlabels=TRUE, ylim=c(0, 100)) El gráfico anterior muestra que hay una diferencia muy grande entre la varianza retenida por la 1er componente principal y el resto de las variables. Dependiendo del objetivo del análisis, podrá elegirse el numero adecuado de componentes a retener, no obstante, la literatura sugiere retener 1 o 2 componentes principales. Es posible realizar el proceso de componentes principales y elegir una de las dos opciones siguientes: Especificar el número de componentes a retener Indicar el porcentaje de varianza a alcanzar La segunda opción elegirá tantas componentes como sean necesarias hasta alcanzar el hiperparámetro mínimo indicado. A continuación se ejemplifica: Caso 1: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,num_comp=2) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 6 ## CVE_ENT GM NOM_ENT IM PC1 PC2 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 ## 4 04 Alto Campeche 0.434 1.12 -0.819 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 ## 10 10 Medio Durango 0.0525 0.135 0.675 ## # … with 22 more rows ## # ℹ Use `print(n = ...)` to see more rows Caso 2: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,threshold=0.90) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 8 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 0.372 0.492 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 -0.0597 -0.492 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 -0.683 -0.400 ## 4 04 Alto Campeche 0.434 1.12 -0.819 -0.151 -0.271 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 0.157 -0.133 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 1.36 -0.0122 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 -1.27 0.633 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 0.159 -0.453 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 ## # … with 22 more rows ## # ℹ Use `print(n = ...)` to see more rows Así es como usaremos el análisis de componentes principales para mejorar la estructura de variables que sirven de input para cualquiera de los modelos posteriores. Continuaremos con un paso más de pre-procesamiento antes de comenzar a aprender nuevos modelos. 3.5 Representación gráfica Uno de los gráficos más famosos al usar CPA es el Biplot, el cual busca representar en dos dimensiones el comportamiento y grado de asociación tanto entre las variables como las observaciones El siguiente biplot permite analizar en dos dimensiones la relación entre variables fviz_pca_var( res.pca, col.var = &quot;contrib&quot;, # Color by contributions to the PC gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) En este gráfico se logra apreciar que todas las características están asociadas a la misma dirección en relación con la primer componente principal, mientras que en la segunda componente existe un poco de diferencia. Entre más pequeño sea el ángulo que separa a dos vectores, mayor correlación existe entre las características. Adicionalmente, si el grado es cercano a los 180°, esto representa una perfecta correlación negativa. Por último, grados cercanos a los 90° ocurren cuando las variables son independientes. Entre más larga es la longitud de cada vector, esto representa mayor varianza en la característica que se está observando. A continuación, se muestra otra versión del biplot en donde se analizan tanto las variables como las observaciones fviz_pca_biplot( res.pca, repel = TRUE, col.var = &quot;#2E9FDF&quot;, # Variables color col.ind = &quot;#696969&quot; # Individuals color ) La proyección ortogonal de cada punto en un vector permite conocer el grado positivo o negativo de una observación en relación con cada una de las variables. Por ejemplo… La proyección del Distrito Federal con cada una de las variables se encuentra en la dirección negativa, lo que implica que la relación de esta entidad con las variables que miden pobreza es negativa. En el caso de Oaxaca, la relación es positiva, lo que implica que Oaxaca tiene una marginación alta. A partir de estas gráficas, se logran realizar simplificaciones o variaciones de gráficas para estudiar posibles agrupaciones, como se muestra en el siguiente gráfico. library(ggrepel) juice(pca_recipe) %&gt;% mutate(GM = factor(GM, levels = c(&quot;Muy alto&quot;, &quot;Alto&quot;, &quot;Medio&quot;, &quot;Bajo&quot;, &quot;Muy bajo&quot;)), ordered = T) %&gt;% ggplot(aes(PC1, PC2, label = NOM_ENT)) + geom_point(aes(color = GM), alpha = 0.7, size = 2) + geom_text_repel() + ggtitle(&quot;Grado de marginación de entidades&quot;) Finalmente, podemos observar como (de izquierda a derecha) los estados con grado de marginación Muy bajo, Bajo, Medio, Alto y Muy Alto respectivamente. juice(pca_recipe) %&gt;% ggplot(aes(x = IM, y = PC1)) + geom_smooth(method = &quot;lm&quot;) + geom_point(size = 2) + ggtitle(&quot;Comparación: Índice Marginación Vs PCA CP1&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.6 Predicciones Es bastante común que sea necesario ajustar o crear un análisis de componentes con la información disponible hasta cierto momento y posteriormente, nuevas observaciones son incorporadas al estudio. Sería incorrecto volver a crear el análisis cada vez que una observación nueva llega. Otro escenario de interés se da cuando se tiene muchísima información (millones de datos) y resulta costoso computacionalmente estar creando el modelo usando toda la información. En ambos escenarios, la solución está en realizar el análisis de componentes principales con una muestra de los datos y posteriormente realizar la predicción de nuevas observaciones ya teniendo creado el modelo El siguiente ejemplo contiene datos de atletas y su desempeño en competencias deportivas. glimpse(decathlon2) ## Rows: 27 ## Columns: 13 ## $ X100m &lt;dbl&gt; 11.04, 10.76, 11.02, 11.34, 11.13, 10.83, 11.64, 11.37, 1… ## $ Long.jump &lt;dbl&gt; 7.58, 7.40, 7.23, 7.09, 7.30, 7.31, 6.81, 7.56, 6.97, 7.2… ## $ Shot.put &lt;dbl&gt; 14.83, 14.26, 14.25, 15.19, 13.48, 13.76, 14.57, 14.41, 1… ## $ High.jump &lt;dbl&gt; 2.07, 1.86, 1.92, 2.10, 2.01, 2.13, 1.95, 1.86, 1.95, 1.9… ## $ X400m &lt;dbl&gt; 49.81, 49.37, 48.93, 50.42, 48.62, 49.91, 50.14, 51.10, 4… ## $ X110m.hurdle &lt;dbl&gt; 14.69, 14.05, 14.99, 15.31, 14.17, 14.38, 14.93, 15.06, 1… ## $ Discus &lt;dbl&gt; 43.75, 50.72, 40.87, 46.26, 45.67, 44.41, 47.60, 44.99, 4… ## $ Pole.vault &lt;dbl&gt; 5.02, 4.92, 5.32, 4.72, 4.42, 4.42, 4.92, 4.82, 4.72, 4.6… ## $ Javeline &lt;dbl&gt; 63.19, 60.15, 62.77, 63.44, 55.37, 56.37, 52.33, 57.19, 5… ## $ X1500m &lt;dbl&gt; 291.70, 301.50, 280.10, 276.40, 268.00, 285.10, 262.10, 2… ## $ Rank &lt;int&gt; 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7,… ## $ Points &lt;int&gt; 8217, 8122, 8067, 8036, 8004, 7995, 7802, 7733, 7708, 765… ## $ Competition &lt;fct&gt; Decastar, Decastar, Decastar, Decastar, Decastar, Decasta… Se eliminarán las últimas 3 columnas para trabajar exclusivamente con las que hacer referencia al desempeño en cada una de las actividades deportivas. decathlon2_train &lt;- decathlon2[1:23, 1:10] glimpse(decathlon2_train) ## Rows: 23 ## Columns: 10 ## $ X100m &lt;dbl&gt; 11.04, 10.76, 11.02, 11.34, 11.13, 10.83, 11.64, 11.37, 1… ## $ Long.jump &lt;dbl&gt; 7.58, 7.40, 7.23, 7.09, 7.30, 7.31, 6.81, 7.56, 6.97, 7.2… ## $ Shot.put &lt;dbl&gt; 14.83, 14.26, 14.25, 15.19, 13.48, 13.76, 14.57, 14.41, 1… ## $ High.jump &lt;dbl&gt; 2.07, 1.86, 1.92, 2.10, 2.01, 2.13, 1.95, 1.86, 1.95, 1.9… ## $ X400m &lt;dbl&gt; 49.81, 49.37, 48.93, 50.42, 48.62, 49.91, 50.14, 51.10, 4… ## $ X110m.hurdle &lt;dbl&gt; 14.69, 14.05, 14.99, 15.31, 14.17, 14.38, 14.93, 15.06, 1… ## $ Discus &lt;dbl&gt; 43.75, 50.72, 40.87, 46.26, 45.67, 44.41, 47.60, 44.99, 4… ## $ Pole.vault &lt;dbl&gt; 5.02, 4.92, 5.32, 4.72, 4.42, 4.42, 4.92, 4.82, 4.72, 4.6… ## $ Javeline &lt;dbl&gt; 63.19, 60.15, 62.77, 63.44, 55.37, 56.37, 52.33, 57.19, 5… ## $ X1500m &lt;dbl&gt; 291.70, 301.50, 280.10, 276.40, 268.00, 285.10, 262.10, 2… En este ejercicio se usará la librería stats y la función prcomp para llevar a cabo el análisis. res.pca &lt;- stats::prcomp(decathlon2_train, scale = TRUE) res.pca ## Standard deviations (1, .., p=10): ## [1] 2.0308159 1.3559244 1.1131668 0.9052294 0.8375875 0.6502944 0.5500742 ## [8] 0.5238988 0.3939758 0.3492435 ## ## Rotation (n x k) = (10 x 10): ## PC1 PC2 PC3 PC4 PC5 ## X100m -0.418859080 0.13230683 -0.27089959 0.03708806 -0.2321476 ## Long.jump 0.391064807 -0.20713320 0.17117519 -0.12746997 0.2783669 ## Shot.put 0.361388111 -0.06298590 -0.46497777 0.14191803 -0.2970589 ## High.jump 0.300413236 0.34309742 -0.29652805 0.15968342 0.4807859 ## X400m -0.345478567 -0.21400770 -0.25470839 0.47592968 0.1240569 ## X110m.hurdle -0.376265119 0.01824645 -0.40325254 -0.01866477 0.2676975 ## Discus 0.365965721 -0.03662510 -0.15857927 0.43636361 -0.4873988 ## Pole.vault -0.106985591 -0.59549862 -0.08449563 -0.37447391 -0.2646712 ## Javeline 0.210864329 -0.28475723 -0.54270782 -0.36646463 0.2361698 ## X1500m 0.002106782 -0.57855748 0.19715884 0.49491281 0.3142987 ## PC6 PC7 PC8 PC9 PC10 ## X100m 0.054398099 -0.16604375 -0.19988005 -0.76924639 0.12718339 ## Long.jump -0.051865558 -0.28056361 -0.75850657 -0.13094589 0.08509665 ## Shot.put -0.368739186 -0.01797323 0.04649571 0.12129309 0.62263702 ## High.jump -0.437716883 0.05118848 0.16111045 -0.28463225 -0.38244596 ## X400m -0.075796432 0.52012255 -0.44579641 0.20854176 -0.09784197 ## X110m.hurdle 0.004048005 -0.67276768 -0.01592804 0.41058421 -0.04475363 ## Discus 0.305315353 -0.25946615 -0.07550934 0.03391600 -0.49418361 ## Pole.vault -0.503563524 -0.01889413 0.06282691 -0.06540692 -0.39288155 ## Javeline 0.556821016 0.24281145 0.10086127 -0.10268134 -0.01103627 ## X1500m 0.064663250 -0.20245828 0.37119711 -0.25950868 0.17991689 Las 10 variables anteriores son las componentes principales creadas a partir de las variables originales. Se puede apreciar en la siguiente línea el impacto que tiene cada componente en términos de varianza. summary(res.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.0308 1.3559 1.1132 0.90523 0.83759 0.65029 0.55007 ## Proportion of Variance 0.4124 0.1839 0.1239 0.08194 0.07016 0.04229 0.03026 ## Cumulative Proportion 0.4124 0.5963 0.7202 0.80213 0.87229 0.91458 0.94483 ## PC8 PC9 PC10 ## Standard deviation 0.52390 0.39398 0.3492 ## Proportion of Variance 0.02745 0.01552 0.0122 ## Cumulative Proportion 0.97228 0.98780 1.0000 fviz_eig(res.pca, addlabels=TRUE, ylim=c(0, 100)) Ahora considérese nuevas observaciones que desean re calculadas usando el análisis de componentes principales. Es indispensable contar con la información de las 10 variables con las que se construyó el modelo. decathlon2_new &lt;- decathlon2[24:27, 1:10] decathlon2_new[, 1:6] ## X100m Long.jump Shot.put High.jump X400m X110m.hurdle ## KARPOV 11.02 7.30 14.77 2.04 48.37 14.09 ## WARNERS 11.11 7.60 14.31 1.98 48.68 14.23 ## Nool 10.80 7.53 14.26 1.88 48.81 14.80 ## Drews 10.87 7.38 13.07 1.88 48.51 14.01 Finalmente, se hace uso de la función predict para calcular el valor de las componentes principales en cada uno de los nuevo individuos: decathlon2_new_coord &lt;- predict(res.pca, newdata = decathlon2_new) decathlon2_new_coord[, 1:4] ## PC1 PC2 PC3 PC4 ## KARPOV 0.7772521 -0.76237804 1.5971253 1.6863286 ## WARNERS -0.3779697 0.11891968 1.7005146 -0.6908084 ## Nool -0.5468405 -1.93402211 0.4724184 -2.2283706 ## Drews -1.0848227 -0.01703198 2.9818031 -1.5006207 Estos resultados pueden integrarse a las gráficas realizadas para comparar su ubicación en relación con los datos originales usados cuando se creó el modelo. # Gráfico con observaciones originales en biplot p &lt;- fviz_pca_ind(res.pca, repel = TRUE) # Se agregan nuevas observaciones fviz_add(p, decathlon2_new_coord, color =&quot;blue&quot;, repel = T) 3.7 Ejercicios Observar en una gráfica la contribución de cada variable a las componentes principales y tratar de interpretar las primeras 2 o 3. Comparar de manera numérica y gráfica la asociación entre los puntos originales asociados a cada atleta y la primer componente principal "],["clustering-no-jerárquico.html", "Capítulo 4 Clustering No Jerárquico 4.1 Cálculo de distancia 4.2 K - means 4.3 Partitioning Around Medoids (PAM) 4.4 Clustering Large Applications (CLARA) 4.5 DBSCAN 4.6 Comparación de algoritmos", " Capítulo 4 Clustering No Jerárquico 4.1 Cálculo de distancia Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación. \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] La distancia de Jaccard es ampliamente usada para medir similitud cuando se trata de variables categóricas. Es usado en análisis de texto y sistemas de recomendación. \\[d(X, Y) = \\frac{X \\cap Y}{X \\cup Y}\\] La distancia de Gower´s mide la similitud entre variables de forma distinta dependiendo del tipo de dato (numérica, nominal, ordinal). \\[D_{Gower}(X_1, X_2) = 1 - \\frac{1}{p} \\sum_{j=1}^{p}{s_j(X_1, X_2)} ; \\quad \\quad s_j(X_1, X_2)=1-\\frac{|y_{1j}-y_{2j}|}{R_j} \\] Para mayor detalle sobre el funcionamiento de la métrica, revisar el siguiente link Un link interesante Otro link interesante 4.1.1 Distancias homogéneas Las distancias basadas en la correlación son ampliamente usadas en múltiples análisis. La función get_dist() puede ser usada para calcular la distancia basada en correlación. Esta medida puede calcularse mediante pearson, spearman o kendall. library(&quot;factoextra&quot;) USArrests_scaled &lt;- scale(USArrests) dist.cor &lt;- get_dist(USArrests_scaled, method = &quot;pearson&quot;) round(as.matrix(dist.cor)[1:7, 1:7], 1) ## Alabama Alaska Arizona Arkansas California Colorado Connecticut ## Alabama 0.0 0.7 1.4 0.1 1.9 1.7 1.7 ## Alaska 0.7 0.0 0.8 0.4 0.8 0.5 1.9 ## Arizona 1.4 0.8 0.0 1.2 0.3 0.6 0.8 ## Arkansas 0.1 0.4 1.2 0.0 1.6 1.4 1.9 ## California 1.9 0.8 0.3 1.6 0.0 0.1 0.7 ## Colorado 1.7 0.5 0.6 1.4 0.1 0.0 1.0 ## Connecticut 1.7 1.9 0.8 1.9 0.7 1.0 0.0 4.1.2 Distancias mixtas La naturaleza los datos es muy distinta. Existen datos numéricos, nominales y ordinales que requieren de un procesamiento distinto. Es de particular interés analizar la distancia entre observaciones cuando se trata de variables con diferente naturaleza: numérico - numérico numérico - nominal numérico - ordinal nominal - nominal nominal - ordinal Existe una función en R que detecta la naturaleza de cada variable y calcula la asociación entre individuos. library(cluster) library(dplyr) data(flower) glimpse(flower) ## Rows: 18 ## Columns: 8 ## $ V1 &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0 ## $ V2 &lt;fct&gt; 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 ## $ V3 &lt;fct&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1 ## $ V4 &lt;fct&gt; 4, 2, 3, 4, 5, 4, 4, 2, 3, 5, 5, 1, 1, 4, 3, 4, 2, 2 ## $ V5 &lt;ord&gt; 3, 1, 3, 2, 2, 3, 3, 2, 1, 2, 3, 2, 2, 2, 2, 2, 2, 1 ## $ V6 &lt;ord&gt; 15, 3, 1, 16, 2, 12, 13, 7, 4, 14, 8, 9, 6, 11, 10, 18, 17, 5 ## $ V7 &lt;dbl&gt; 25, 150, 150, 125, 20, 50, 40, 100, 25, 100, 45, 90, 20, 80, 40, 20… ## $ V8 &lt;dbl&gt; 15, 50, 50, 50, 15, 40, 20, 15, 15, 60, 10, 25, 10, 30, 20, 60, 60,… Como puede observarse, cada una de las variables anteriores tiene una naturaleza distinta. La función daisy() calcula la matriz de disimilaridades de acuerdo con la metodología correspondiente a cada par de variables. dd &lt;- daisy(flower) round(as.matrix(dd)[1:10, 1:10], 2) ## 1 2 3 4 5 6 7 8 9 10 ## 1 0.00 0.89 0.53 0.35 0.41 0.23 0.29 0.42 0.58 0.61 ## 2 0.89 0.00 0.51 0.55 0.62 0.66 0.60 0.46 0.43 0.45 ## 3 0.53 0.51 0.00 0.57 0.37 0.30 0.49 0.60 0.45 0.47 ## 4 0.35 0.55 0.57 0.00 0.64 0.42 0.34 0.30 0.81 0.56 ## 5 0.41 0.62 0.37 0.64 0.00 0.34 0.42 0.47 0.33 0.38 ## 6 0.23 0.66 0.30 0.42 0.34 0.00 0.19 0.57 0.51 0.41 ## 7 0.29 0.60 0.49 0.34 0.42 0.19 0.00 0.41 0.59 0.59 ## 8 0.42 0.46 0.60 0.30 0.47 0.57 0.41 0.00 0.64 0.66 ## 9 0.58 0.43 0.45 0.81 0.33 0.51 0.59 0.64 0.00 0.43 ## 10 0.61 0.45 0.47 0.56 0.38 0.41 0.59 0.66 0.43 0.00 4.1.3 Visualización de distancias En cualquier análisis, es de gran valor contar con un gráfico que permita conocer de manera práctica y simple el resumen de distancias. Un mapa de calor es una solución bastante útil, el cual representará de en una escala de color a los elementos cerca y lejos. fviz_dist(dist.cor) El nivel del color es proporcional al valor de disimilaridad entre observaciones. Cuando la distancia es cero, el color es rojo puro y cuando la distancia es amplia, el color es azul puro. Los elementos que pertenecen a un mismo cluster se muestran en orden consecutivo. 4.2 K - means La agrupación en grupos con K-means es uno de los algoritmos de aprendizaje de máquina no supervisados más simples y populares. K-medias es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Un cluster se refiere a una colección de puntos de datos agregados a a un grupo debido a ciertas similitudes. 4.2.1 Ajuste de modelo: ¿Cómo funciona el algortimo? Paso 1: Seleccionar el número de clusters K El primer paso en k-means es elegir el número de conglomerados, K. Como estamos en un problema de análisis no supervisado, no hay K correcto, existen métodos para seleccionar algún K pero no hay respuesta correcta. Paso 2: Seleccionar K puntos aleatorios de los datos como centroides. A continuación, seleccionamos aleatoriamente el centroide para cada grupo. Supongamos que queremos tener 2 grupos, por lo que K es igual a 2, seleccionamos aleatoriamente los centroides: Paso 3: Asignamos todos los puntos al centroide del cluster más cercano. Una vez que hemos inicializado los centroides, asignamos cada punto al centroide del cluster más cercano: Paso 4: Volvemos a calcular los centroides de los clusters recién formados. Ahora, una vez que hayamos asignado todos los puntos a cualquiera de los grupos, el siguiente paso es calcular los centroides de los grupos recién formados: Paso 5: Repetir los pasos 3 y 4. Criterios de paro: Existen tres criterios de paro para detener el algoritmo: Los centroides de los grupos recién formados no cambian: Podemos detener el algoritmo si los centroides no cambian. Incluso después de múltiples iteraciones, si obtenemos los mismos centroides para todos los clusters, podemos decir que el algoritmo no está aprendiendo ningún patrón nuevo y es una señal para detener el entrenamiento. Los puntos permanecen en el mismo grupo: Otra señal clara de que debemos detener el proceso de entrenamiento si los puntos permanecen en el mismo cluster incluso después de entrenar el algoritmo para múltiples iteraciones. Se alcanza el número máximo de iteraciones: Finalmente, podemos detener el entrenamiento si se alcanza el número máximo de iteraciones. Supongamos que hemos establecido el número de iteraciones en 100. El proceso se repetirá durante 100 iteraciones antes de detenerse. 4.2.2 Calidad de ajuste 4.2.2.1 Inercia La idea detrás de la agrupación de k-medias consiste en definir agrupaciones de modo que se minimice la variación total dentro de la agrupación (conocida como within cluster variation o inertia). Existen distintos algoritmos de k-medias, el algoritmo estándar es el algoritmo de Hartigan-Wong, que define within cluster variation como la suma de las distancias euclidianas entre los elementos y el centroide correspondiente al cuadrado: \\[W(C_k)=\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] donde \\(x_i\\) es una observación que pertenece al cluster \\(C_k\\) y \\(\\mu_k\\) es la media del cluster \\(C_k\\) Cada observación \\(x_i\\) se asigna a un grupo de modo que la suma de cuadrados de la distancia de la observación a sus centroide del grupo asignado \\(\\mu_k\\) es mínima. Definimos la total within cluster variation total de la siguiente manera: \\[total \\quad within = \\sum_{k=1}^{K}W(C_k) = \\sum_{k=1}^{K}\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] 4.2.2.2 Silhouette El índice de Silhouette hace referencia al método de interpretación y validación de la consistencia dentro de los conglomerados. La medida de Silhouette es una medida de qué tan similar es un elemento al interior del cluster en comparación con otros. El rango del índice va de -1 a 1, donde un alto valor del índice indica que el objeto es consistente dentro de su cluster y pobremente asociado a otro conglomerado. Si muchos elementos presentan un valor bajo o negativo, entonces la configuración del cluster puede que tenga muy pocos o demasiados conglomerados. El índice de Silhouette puede ser calculado con cualquier métrica de distancia, tal como la distancia Euclidiana, Manhattan u otro. Se define al índice de Silhouette como: \\[s(i)=\\frac{b(i)-a(i)}{max\\left\\{a(i), b(i)\\right\\}}; \\quad si \\quad|C_i|&gt;1; \\quad -1 \\le s(i) \\le 1\\] \\[s(i)=0; \\quad si \\quad|C_i|=1\\] Asumiendo que cada elemento ha sido asignado a un cluster, para cada punto i en \\(C_i\\), entonces: \\[a(i)=\\frac{1}{|C_i|-1}\\sum_{j\\in C_i, i \\neq j}d(i,j)\\] \\(a(i)\\) = Es la distancia promedio entre i y el resto de elementos dentro del conglomerado i. \\(|C_i|\\) = Es el número de elementos dentro del i-ésimo conglomerado. \\(d(i,j)\\) = Es la distancia entre los elementos i y j en el cluster \\(C_i\\). Se puede interpretar a \\(a(i)\\) como una métrica de qué tan bien asignado está el elemento i al conglomerado. Entre más pequeño sea el valor, mejor asignado se encuentra. Posteriormente, se calcula la medida de disimilaridad del punto i a algún conglomerado \\(C_k\\) como el promedio de la distancia del punto i a todos los puntos en \\(C_k\\) \\((C_k \\neq C_i)\\). Para cada punto \\(i \\in C_i\\), se define: \\[b(i)=min_{k\\neq i}\\frac{1}{|C_k|}\\sum_{j\\in C_k}d(i,j)\\] ser la distancia media más pequeña (de ahí el operador min en la fórmula) de i a todos los puntos en cualquier otro grupo, del cual i no es miembro. Se dice que el conglomerado con esta disimilitud media más pequeña es el “conglomerado vecino” de i porque es el siguiente conglomerado que mejor se ajusta para el punto i. 4.2.3 ¿Cómo seleccionamos K? Una de las dudas más comunes que se tienen al trabajar con K-Means es seleccionar el número correcto de clusters. El número máximo posible de conglomerados será igual al número de observaciones en el conjunto de datos. Pero entonces, ¿cómo podemos decidir el número óptimo de agrupaciones? Una cosa que podemos hacer es trazar un gráfico, también conocido como gráfica de codo, donde el eje x representará el número de conglomerados y el eje y será una métrica de evaluación, en este caso usaremos inertia. Comenzaremos con un valor de K pequeño, digamos 2. Entrenaremos el modelo usando 2 grupos, calculamos la inercia para ese modelo y, finalmente, agregamos el punto en el gráfico mencionado. Digamos que tenemos un valor de inercia de alrededor de 1000: Ahora, aumentaremos el número de conglomerados, entrenaremos el modelo nuevamente y agregaremos el valor de inercia en la gráfica con distintos números de K: Cuando cambiamos el valor de K de 2 a 4, el valor de inercia se redujo de forma muy pronunciada. Esta disminución en el valor de inercia se reduce y eventualmente se vuelve constante a medida que aumentamos más el número de grupos. Entonces, el valor de K donde esta disminución en el valor de inercia se vuelve constante se puede elegir como el valor de grupo correcto para nuestros datos. Aquí, podemos elegir cualquier número de conglomerados entre 6 y 10. Podemos tener 7, 8 o incluso 9 conglomerados. También debe tener en cuenta el costo de cálculo al decidir la cantidad de clusters. Si aumentamos el número de clusters, el costo de cálculo también aumentará. Entonces, si no tiene recursos computacionales altos, deberíamos un número menor de clusters. 4.2.4 Implementación en R Usaremos los datos USArrests, que contiene estadísticas, en arrestos por cada 100,000 residentes por asalto, asesinato y violación en cada uno de los 50 estados de EE. UU. En 1973. También se da el porcentaje de la población que vive en áreas urbanas. data(&quot;USArrests&quot;) head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 df &lt;-scale(USArrests, center = T, scale = T) df &lt;- na.omit(df) head(df, n = 5) ## Murder Assault UrbanPop Rape ## Alabama 1.24256408 0.7828393 -0.5209066 -0.003416473 ## Alaska 0.50786248 1.1068225 -1.2117642 2.484202941 ## Arizona 0.07163341 1.4788032 0.9989801 1.042878388 ## Arkansas 0.23234938 0.2308680 -1.0735927 -0.184916602 ## California 0.27826823 1.2628144 1.7589234 2.067820292 Usaremos la función kmeans(), los siguientes parámetros son los más usados: X: matriz numérica de datos, o un objeto que puede ser forzado a tal matriz (como un vector numérico o un marco de datos con todas las columnas numéricas). centers: ya sea el número de conglomerados(K), o un conjunto de centros de conglomerados iniciales (distintos). Si es un número, se elige un conjunto aleatorio de observaciones (distintas) en x como centros iniciales. iter.max: el número máximo de iteraciones permitido. nstart: si centers es un número, ¿cuántos conjuntos aleatorios deben elegirse? algorithm: Algoritmo a usar En el siguiente ejemplo se agruparán los datos en seis grupos (centers = 6). Como se había mencionado, la función kmeans también tiene una opción nstart que intenta múltiples configuraciones iniciales y regresa la mejor, agregar nstart = 25 generará 25 configuraciones iniciales. k6 &lt;- kmeans(df, centers = 6, nstart = 25) dplyr::glimpse(k6) ## List of 9 ## $ cluster : Named int [1:50] 4 3 1 2 3 3 5 2 1 4 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... ## $ centers : num [1:6, 1:4] 0.867 -0.216 0.456 1.58 -0.696 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Murder&quot; &quot;Assault&quot; &quot;UrbanPop&quot; &quot;Rape&quot; ## $ totss : num 196 ## $ withinss : num [1:6] 5.89 10.86 6.26 6.13 5.24 ... ## $ tot.withinss: num 42.9 ## $ betweenss : num 153 ## $ size : int [1:6] 8 13 4 7 7 11 ## $ iter : int 3 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; La salida de kmeans es una lista con distinta información. La más importante: cluster: Un vector de números enteros (de 1:K) que indica el grupo al que se asigna cada punto. centers: una matriz de centros. totss: La suma total de cuadrados. withinss: Vector de suma de cuadrados dentro del grupo, un componente por grupo. tot.withinss: Suma total de cuadrados dentro del conglomerado, es decir, sum(withinss) betweenss: La suma de cuadrados entre grupos, es decir, \\(totss-tot.withinss\\). size: el número de observaciones en cada grupo. También podemos ver nuestros resultados usando la función fviz_cluster(). Esto proporciona una ilustración de los grupos. Si hay más de dos dimensiones (variables), fviz_cluster() realizará un análisis de componentes principales (PCA) y trazará los puntos de datos de acuerdo con los dos primeros componentes principales que explican la mayor parte de la varianza. library(factoextra) fviz_cluster(k6, data = df, ellipse.type = &quot;t&quot;, repel = TRUE) Debido a que el número de conglomerados (K) debe establecerse antes de iniciar el algoritmo, a menudo es recomendado utilizar varios valores diferentes de K y examinar las diferencias en los resultados. Podemos ejecutar el mismo proceso para 3, 4 y 5 clusters, y los resultados se muestran en la siguiente figura: library(patchwork) library(gridExtra) k2 &lt;- kmeans(df, centers = 2, nstart = 25) k3 &lt;- kmeans(df, centers = 3, nstart = 25) k4 &lt;- kmeans(df, centers = 4, nstart = 25) k5 &lt;- kmeans(df, centers = 5, nstart = 25) p2 &lt;- fviz_cluster(k2, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 2&quot;) p3 &lt;- fviz_cluster(k3, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 3&quot;) p4 &lt;- fviz_cluster(k4, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 4&quot;) p5 &lt;- fviz_cluster(k5, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 5&quot;) grid.arrange(p2, p3, p4, p5, nrow = 2) Recordemos que podemos usar la gráfica de codo para obtener el número óptimo de K, usaremos la función fviz_nbclust() para esto. set.seed(123) wss_plot &lt;- fviz_nbclust(df, kmeans, method = &quot;wss&quot;) set.seed(123) sil_plot &lt;- fviz_nbclust(df, kmeans, method = &quot;silhouette&quot;) wss_plot + sil_plot Con las gráficas anteriores no es claro cual K debemos elegir, podemos usar la función NbClust() de la librería NbClust para comparar 30 indices distintos y ver cual es el mejor K. library(NbClust) nb &lt;- NbClust( data = df, diss = NULL, distance = &quot;euclidean&quot;, min.nc = 2, method = &quot;kmeans&quot; ) fviz_nbclust(nb) ## Among all indices: ## =================== ## * 2 proposed 0 as the best number of clusters ## * 9 proposed 2 as the best number of clusters ## * 2 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 1 proposed 5 as the best number of clusters ## * 4 proposed 6 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 5 proposed 14 as the best number of clusters ## * 1 proposed 15 as the best number of clusters ## ## Conclusion ## ========================= ## * According to the majority rule, the best number of clusters is 2 . Con ayuda de la función anterior, observamos que 9 indices proponen \\(K=2\\) set.seed(123) final &lt;- kmeans(df, 2, nstart = 25) kmeans_plot &lt;- fviz_cluster( final, data = df, ellipse.type = &quot;t&quot;, repel = TRUE) + ggtitle(&quot;K-Means Plot&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) kmeans_plot 4.2.5 Warnings Dado que este algoritmo está basado en promedios, debe de ser considerada su sensibilidad a valores atípicos, esto es, si un valor esta lejos del resto, el centroide de un cluster puede cambiar drásticamente y eso significa que también puede incluir dentro del mismo grupo a puntos diferentes de los que de otra manera no serían incluidos en ese conglomerado. 4.3 Partitioning Around Medoids (PAM) El algoritmo k-medoides es un enfoque de agrupamiento para particionar un conjunto de datos en k grupos o clusters. En k-medoides, cada grupo está representado por uno de los puntos de datos pertenecientes a un grupo. Estos puntos son nombrados medoides. El término medoide se refiere a un objeto dentro de un grupo para el cual la disimilitud promedio entre él y todos los demás miembros del cluster son mínimos. Corresponde a el punto más céntrico del grupo. Este algoritmo es una alternativa sólida de k-medias. Debido a que este algoritmo es menos sensible al ruido y los valores atípicos, en comparación con k-medias, pues usa medoides como centros de conglomerados en lugar de medias. El uso de medias implica que la agrupación de k-medias es muy sensible a los valores atípicos, lo cual puede afectar gravemente la asignación de observaciones a los conglomerados. El método de agrupamiento de k-medoides más común es el algoritmo PAM (Partitioning Around Medoids, Kaufman &amp; Rousseeuw, 1990). 4.3.1 Algoritmo PAM El algoritmo PAM se basa en la búsqueda de k objetos representativos o medoides entre las observaciones del conjunto de datos. Después de encontrar un conjunto de k medoides, los grupos se construyen asignando cada observación al medoide más cercano. Posteriormente, cada medoide m y cada punto de datos no medoide seleccionado se intercambian y se calcula la función objetivo. La función objetivo corresponde a la suma de las disimilitudes de todos los objetos a su medoide más cercano. El objetivo es encontrar k objetos representativos que minimicen la suma de disimilitudes de las observaciones con su objeto representativo más cercano. Como se mencionó anteriormente, el algoritmo PAM funciona con una matriz de disimilitud y para calcular esta matriz, el algoritmo puede utilizar dos métricas: La distancia euclidiana, que es la raíz de la suma de cuadrados de las diferencias; Y la distancia de Manhattan, que es la suma de distancias absolutas. Nota: En la práctica, se debería obtener resultados similares la mayor parte del tiempo, utilizando cualquiera de estas distancias mencionadas. Si lo datos contienen valores atípicos, distancia de Manhattan debería dar resultados más sólidos, mientras que la distancia euclidiana se vería influenciada por valores inusuales. 4.3.2 Implementación en R Para estimar el número óptimo de clusters, usaremos el método de silueta promedio. La idea es calcular el algoritmo PAM utilizando diferentes valores de los conglomerados k. Después, la silueta promedio de los conglomerados se dibuja de acuerdo con el número de conglomerados. La silueta media mide la calidad de un agrupamiento. Una silueta media alta indica una buena agrupación. El número óptimo de conglomerados k es el que maximiza la silueta promedio sobre un rango de valores posibles para k La función fviz_nbclust() del paquete factoextra proporciona una solución conveniente para estimar el número óptimo de conglomerados con diferentes métodos. library(cluster) # Elbow method Elbow &lt;- fviz_nbclust(df, pam, method = &quot;wss&quot;) + geom_vline(xintercept = 4, linetype = 2)+ labs(subtitle = &quot;Elbow method&quot;) # Silhouette method Silhouette &lt;- fviz_nbclust(df, pam, method = &quot;silhouette&quot;) + labs(subtitle = &quot;Silhouette method&quot;) Elbow + Silhouette library(&quot;NbClust&quot;) nb &lt;- NbClust(df, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 10, method = &quot;median&quot;) fviz_nbclust(nb) ## Among all indices: ## =================== ## * 2 proposed 0 as the best number of clusters ## * 1 proposed 1 as the best number of clusters ## * 8 proposed 2 as the best number of clusters ## * 1 proposed 3 as the best number of clusters ## * 1 proposed 5 as the best number of clusters ## * 1 proposed 7 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 10 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## ## Conclusion ## ========================= ## * According to the majority rule, the best number of clusters is 9 . ¿Qué se hace en estos casos? Es importante conocer las diferencias entre cada grupo y entender si este resultado tiene sentido desde un enfoque de negocio. Una vez entendiendo la razón por la cual 9 grupos es viable desde múltiples criterios, se deberá tomar la decisión del número adecuado de grupos. Otro criterio importante, es entender los índices y su interpretación. Conocer el significado de cada índice será importante para conocer la validez de su uso y consideración. A partir de la gráfica se observa que la cantidad sugerida de grupos es 2 o 9, por lo que en la siguiente sección se clasificarán las observaciones en 2 grupos. La función pam() del paquete Cluster se puede utilizar para calcular PAM. k_mediods &lt;- pam(df, 2) print(k_mediods) ## Medoids: ## ID Murder Assault UrbanPop Rape ## New Mexico 31 0.8292944 1.3708088 0.3081225 1.1603196 ## Nebraska 27 -0.8008247 -0.8250772 -0.2445636 -0.5052109 ## Clustering vector: ## Alabama Alaska Arizona Arkansas California ## 1 1 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 2 2 1 1 ## Hawaii Idaho Illinois Indiana Iowa ## 2 2 1 2 2 ## Kansas Kentucky Louisiana Maine Maryland ## 2 2 1 2 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 2 1 2 1 1 ## Montana Nebraska Nevada New Hampshire New Jersey ## 2 2 1 2 2 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 1 2 2 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 2 2 2 2 1 ## South Dakota Tennessee Texas Utah Vermont ## 2 1 1 2 2 ## Virginia Washington West Virginia Wisconsin Wyoming ## 2 2 2 2 2 ## Objective function: ## build swap ## 1.441358 1.368969 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; ## [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; La salida impresa muestra: Los medoides del grupo: una matriz, cuyas filas son los medoides y las columnas son variables El vector de agrupación: un vector de números enteros (de \\(1:k\\)) que indica la agrupación a que se asigna a cada punto Para visualizar los resultados de la partición, usaremos la función fviz_cluster() del paquete factoextra. Esta función dibuja un diagrama de dispersión de puntos de datos coloreados por números de grupo. Si los datos contienen más de 2 variables, se utiliza el algoritmo de análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos. En este caso, los dos primeros componentes se utilizan para trazar los datos. pam_plot &lt;- fviz_cluster( k_mediods, palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;), ellipse.type = &quot;t&quot;, repel = TRUE, ggtheme = theme_minimal()) + ggtitle(&#39;K-Medoids Plot&#39;) + theme(legend.position = &quot;bottom&quot;) pam_plot Ejercicio: Calcular los centroides de cada grupo realizando 2 segmentaciones Calcular los centroides de cada grupo realizando 9 segmentaciones Comparar los resultados anteriores y comentar 4.4 Clustering Large Applications (CLARA) CLARA (Clustering Large Applications) es una extensión al método k-medoides para tratar datos que contienen un gran número de observaciones con el fin de reducir tiempo de computación y problema de almacenamiento de RAM. Esto se logra utilizando muestreo. El algoritmo es el siguiente: Dividir aleatoriamente los conjuntos de datos en varios subconjuntos con tamaño fijo (tamaño de muestra). Calcular el algoritmo PAM en cada subconjunto y elegir k (medoides). Asignar cada observación de los datos completos al medoide más cercano. Calcular la media (o la suma) de las disimilitudes de las observaciones para su medoide más cercano. Esto se usa como una medida de calidad de la agrupación. Conserve el subconjunto de datos para el que la media (o la suma) es mínima. Un análisis más se lleva a cabo en la partición final. Nota: Cada subconjunto de datos está obligado a contener los medoides obtenidos del mejor subconjunto de datos hasta entonces. Las observaciones extraídas al azar se agregan a este conjunto hasta que el tamaño de muestra ha sido alcanzado. 4.4.1 Implementación en R Estimaremos el número óptimo de clusters como se realizó en la sección pasada. fviz_nbclust(df, clara, method = &quot;silhouette&quot;)+ theme_minimal() A partir de la gráfica, la cantidad sugerida de grupos es 2. En el siguiente paso clasificaremos las observaciones en 2 grupos. clara &lt;- clara(df, 2, samples = 50, pamLike = TRUE) print(clara) ## Call: clara(x = df, k = 2, samples = 50, pamLike = TRUE) ## Medoids: ## Murder Assault UrbanPop Rape ## New Mexico 0.8292944 1.3708088 0.3081225 1.1603196 ## Nebraska -0.8008247 -0.8250772 -0.2445636 -0.5052109 ## Objective function: 1.368969 ## Clustering vector: Named int [1:50] 1 1 1 2 1 1 2 2 1 1 2 2 1 2 2 2 2 1 ... ## - attr(*, &quot;names&quot;)= chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; ... ## Cluster sizes: 20 30 ## Best sample: ## [1] Alabama Arizona California Colorado Connecticut ## [6] Delaware Georgia Idaho Illinois Iowa ## [11] Kansas Kentucky Louisiana Maine Maryland ## [16] Massachusetts Michigan Minnesota Mississippi Missouri ## [21] Montana Nebraska Nevada New Hampshire New Mexico ## [26] New York North Carolina North Dakota Ohio Oklahoma ## [31] Oregon Pennsylvania Rhode Island South Carolina South Dakota ## [36] Tennessee Texas Utah Vermont Virginia ## [41] Washington West Virginia Wisconsin Wyoming ## ## Available components: ## [1] &quot;sample&quot; &quot;medoids&quot; &quot;i.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [6] &quot;clusinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;silinfo&quot; &quot;data&quot; Se visualiza ahora los resultados del método CLARA fviz_cluster( clara, palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;), ellipse.type = &quot;t&quot;, ggtheme = theme_minimal() ) 4.5 DBSCAN DBSCAN (agrupación espacial basada en densidad y aplicación con ruido), es un algoritmo de agrupamiento basado en densidad, que puede utilizarse para identificar agrupaciones de cualquier forma en un conjunto de datos que contenga ruido y valores atípicos. La idea básica detrás del enfoque de agrupamiento basado en densidad se deriva de un método de agrupamiento intuitivo. Por ejemplo, mirando la siguiente figura, uno puede identificar fácilmente cuatro grupos junto con varios puntos de ruido, debido a las diferencias en la densidad de puntos. Los clusters son regiones densas en el espacio de datos, separadas por regiones de menor densidad de puntos. El algoritmo DBSCAN se basa en esta noción intuitiva de “clusters” y “ruido.” La idea clave es que para cada punto de un grupo, la vecindad de un determinado radio debe contener al menos un número mínimo de puntos. Los métodos de particionamiento vistos anteriormente son adecuados para encontrar grupos de forma esférica o grupos convexos. En otras palabras, ellos funcionan bien solo para grupos compactos y bien separados. Además, también son severamente afectados por la presencia de ruido y valores atípicos en los datos. Desafortunadamente, los datos de la vida real pueden contener: Grupos de forma arbitraria como los como se muestra en la siguiente figura (grupos ovalados, lineales y en forma de “S”). Muchos valores atípicos y ruido. El gráfico anterior contiene 5 grupos y valores atípicos, que incluyen: 2 clusters ovalados 2 clusters lineales 1 cluster compacto Dados los datos “multishapes” del paquete factoextra, el algoritmo de k-medias tiene dificultades para identificar estos grupos con formas arbitrarias. Para ilustrar esta situación, el siguiente código calcula k-medias en el conjunto de datos mencionado. La función fviz_cluster() del paquete factoextra se utiliza para visualizar los clusters. data(&quot;multishapes&quot;) df &lt;- multishapes[, 1:2] set.seed(123) kmeans &lt;- kmeans(df, 5, nstart = 25) fviz_cluster( kmeans, df, geom = &quot;point&quot;, ellipse= FALSE, show.clust.cent = FALSE, palette = &quot;jco&quot;, ggtheme = theme_minimal() ) Sabemos que hay 5 grupos de en los datos, pero se puede ver que el método de k-medias identifica incorrectamente estos 5 grupos. 4.5.1 Algoritmo El objetivo es identificar regiones densas, que se pueden medir por la cantidad de objetos cerca de un punto dado. Se requieren dos parámetros importantes para DBSCAN: epsilon (“eps”): Define el radio de vecindad alrededor un punto x. puntos mínimos (“MinPts”): Es el número mínimo de vecinos dentro del radio “eps”. Cualquier punto x en el conjunto de datos, con un recuento de vecinos mayor o igual que MinPts, es marcado como un punto central. Decimos que x es un punto fronterizo, si el número de sus vecinos es menos que MinPts, pero pertenece a la vecindad de algún punto central z. Finalmente, si un punto no es ni un núcleo ni un punto fronterizo, entonces se denomina punto de ruido o parte aislada. La siguiente figura muestra los diferentes tipos de puntos (puntos centrales, fronterizos y atípicos) usando MinPts = 6. Aquí x es un punto central porque los vecinos \\(s_{\\epsilon}(x) = 6\\), y es un punto fronterizo ya que \\(s_{\\epsilon}(y) &lt; \\text{ MinPts}\\), pero pertenece a la vecindad del punto central x. Finalmente, z es un punto de ruido. Comenzamos definiendo \\(3\\) términos, necesarios para comprender el algoritmo DBSCAN: Densidad directa alcanzable: Un punto \\(A\\) es directamente de densidad alcanzable desde otro punto \\(B\\) si: \\(A\\) está en la vecindad de \\(B\\) y \\(B\\) es un punto central. Densidad alcanzable: Un punto \\(A\\) es la densidad alcanzable desde \\(B\\) si hay un conjunto de los puntos centrales que van de \\(B\\) a \\(A\\). Densidad conectada: Dos puntos \\(A\\) y \\(B\\) están densamente conectados si hay un punto central \\(C\\), de modo que tanto \\(A\\) como \\(B\\) tienen densidad alcanzable desde \\(C\\). Un cluster basado en densidad se define como un grupo de puntos conectados por densidad. El algoritmo de agrupamiento basado en densidad (DBSCAN) funciona de la siguiente manera: Para cada punto \\(x_i\\), calcular la distancia entre \\(x_i\\) y los otros puntos. Hallar todos los puntos vecinos dentro de la distancia eps del punto de partida (\\(x_i\\)). Cada punto con número de vecinos mayor o igual a MinPts, se marca como punto central o visitado. Para cada punto central, si aún no está asignado a un cluster, crear un nuevo cluster. Encuentrar recursivamente todos sus puntos densamente conectados y asignarlos a el mismo grupo que el punto central. Iterar a través de los puntos no visitados restantes en el conjunto de datos. Los puntos que no pertenecen a ningún cluster se tratan como valores atípicos o ruido. 4.5.2 Estimación de parámetros MinPts: Cuanto mayor sea el conjunto de datos, mayor será el valor de minPts. Deben elegirse al menos 3. \\(\\epsilon\\): El valor de \\(\\epsilon\\) se puede elegir mediante un gráfico de distancia \\(k\\), trazando la distancia al vecino más cercano \\(k = minPts\\). Los buenos valores de \\(\\epsilon\\) son donde el gráfico muestra una fuerte curva. 4.5.2.1 Estimación el valor óptimo de \\(\\epsilon\\) El método consiste en calcular las \\(k\\) distancias vecinas más cercanas en una matriz de puntos. La idea es calcular, el promedio de las distancias de cada punto a su \\(k\\) vecino más cercano. El valor de k será especificado por el usuario y corresponde a MinPts. A continuación, estas k-distancias se trazan en orden ascendente. El objetivo es determinar la “rodilla,” que corresponde al parámetro óptimo de eps. Una “rodilla” corresponde a un umbral donde se produce un cambio brusco a lo largo de la curva. La función kNNdistplot() de el paquete dbscan se puede usar para dibujar la distancia-k. library(dbscan) dbscan::kNNdistplot(df, k = 5) abline(h = 0.15, lty = 2) Se puede ver que el valor óptimo de \\(\\epsilon\\) está alrededor de una distancia de \\(0.15\\). 4.5.3 Implementación en R Utilizaremos el paquete fpc para calcular DBSCAN. También es posible utilizar el paquete dbscan, que proporciona una re-implementación más rápida del algoritmo en comparación con el paquete fpc. library(&quot;fpc&quot;) set.seed(123) db &lt;- fpc::dbscan(df, eps = 0.15, MinPts = 5) fviz_cluster( db, data = df, stand = FALSE, ellipse = FALSE, show.clust.cent = FALSE, geom = &quot;point&quot;, palette = &quot;jco&quot;, ggtheme = theme_minimal() ) Nota: La función fviz_cluster() usa diferentes símbolos de puntos para los puntos centrales (es decir, puntos semilla) y puntos fronterizos. Los puntos negros corresponden a valores atípicos. Puede verse que DBSCAN funciona mejor para estos conjuntos de datos y puede identificar el conjunto correcto de clusters en comparación con los algoritmos de k-medias. Los resultados del algoritmo se pueden ver de la siguiente manera print(db) ## dbscan Pts=1100 MinPts=5 eps=0.15 ## 0 1 2 3 4 5 ## border 31 24 1 5 7 1 ## seed 0 386 404 99 92 50 ## total 31 410 405 104 99 51 En la tabla anterior, los nombres de las columnas son el número de grupo. El grupo \\(0\\) corresponde a valores atípicos (puntos negros en el gráfico DBSCAN). 4.5.4 Ventajas de DBSCAN A diferencia de K-medias, DBSCAN no requiere que el usuario especifique el número de clusters que se generarán. DBSCAN puede encontrar cualquier forma de clusters. No es necesario que el grupo sea circular. DBSCAN puede identificar valores atípicos. 4.5.5 Aplicación DBSCAN Aplicaremos ahora el algoritmo DBSCAN a los datos USArrests. Veamos primero el valor óptimo de \\(\\epsilon\\) para estos datos. data(&quot;USArrests&quot;) df &lt;- scale(USArrests) dbscan::kNNdistplot(df, k = 3) abline(h = 1.175, lty = 1.5) Se puede ver que el valor óptimo de \\(\\epsilon\\) está alrededor de una distancia de \\(1.175\\). set.seed(114234) db &lt;- fpc::dbscan(df, eps = 1.175, MinPts = 2) print(db) ## dbscan Pts=50 MinPts=2 eps=1.175 ## 0 1 2 ## border 6 0 0 ## seed 0 7 37 ## total 6 7 37 dbscan_plot &lt;- fviz_cluster( db, data = df, stand = FALSE, axes = c(1,2), repel = TRUE, show.clust.cent = FALSE, geom = &quot;point&quot;, palette = &quot;jco&quot;, ellipse.type = &quot;t&quot;, ggtheme = theme_minimal()) + ggtitle(&#39;DBSCAN Plot&#39;) + theme(legend.position = &quot;bottom&quot;) dbscan_plot 4.6 Comparación de algoritmos Un buen análisis de clustering no sucede sin antes comparar los resultados producidos por los distintos algoritmos. A continuación, se presenta la comparación de las gráficas. Esta comparación visual sirve de apoyo para conocer las diferencias entre los distintos métodos, sin embargo, esto no sustituye al análisis numérico de wss, silhouette o algún otro. kmeans_plot + pam_plot + dbscan_plot "],["clustering-jerárquico.html", "Capítulo 5 Clustering Jerárquico", " Capítulo 5 Clustering Jerárquico En esta sección se analizarán diferentes metodologías que tienen como propósito realizar segmentaciones de unidades de manera jerárquica, es decir, a partir de un único grupo se van agrupando o separando los individuos dependiendo de qué tan lejanos o cercanos se encuentran unos de otros. Las principales metodologías a revisar serán: Liga simple o vecino más cercano Liga compleja o vecino más lejano Liga promedio Liga centroide Varianza mínima de Ward A partir del concepto de distancia entre puntos en un espacio de N dimensiones (variables), se realiza la agrupación de elementos para posteriormente calcular cuántos grupos es conveniente usar. Este proceso puede ser graficado de múltiples formas, sin embargo, la visualización más usada corresponde al dendograma, el cual es un gráfico como el presentado a continuación: El eje horizontal representa los puntos de datos. La altura a lo largo del eje vertical representa la distancia entre los grupos. Las líneas verticales en el gráfico representan grupos. La altura de estas líneas representa la distancia desde el grupo más cercano. Podemos encontrar el número de conglomerados que mejor representan los grupos en los datos usando el dendrograma. Las líneas verticales con las mayores distancias entre ellas, es decir, la mayor altura en el mismo nivel, dan el número de grupos que mejor representan los datos. "],["anexo-visualización-con-ggplot.html", "Capítulo 6 Anexo: Visualización con Ggplot 6.1 Ggplot2 6.2 Referencias", " Capítulo 6 Anexo: Visualización con Ggplot 6.1 Ggplot2 Comparando con los gráficos base de R, ggplot2: Tiene una gramática más compleja para gráficos simples Tiene una gramática menos compleja para gráficos complejos o muy personalizados Los datos siempre deben ser un data.frame. Usa un sistema diferente para añadir elementos al gráfico. Histograma con los gráficos base: data(iris) hist(iris$Sepal.Length) Histograma con ggplot2: library(ggplot2) ggplot(iris, aes(x = Sepal.Length)) + geom_histogram(color = &#39;white&#39;, bins=8) Ahora vamos a ver un gráfico con colores y varias series de datos. Con los gráficos base: plot(Sepal.Length ~ Sepal.Width, col = factor(Species), data = iris) Con ggplot2: ggplot(iris, aes(x=Sepal.Width , y= Sepal.Length, color=Species))+ geom_point() 6.1.1 Objetos aesteticos En ggplot2, aestético significa “algo que puedes ver.” Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color “externo”) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 6.1.2 Objetos geométricos o capas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para gráficas univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc…) Lineas (geom_line() para series temporales, lineas de tendencia, etc…) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Si queremos conocer la lista de objetos geométricos podemos ejecutar el siguiente código: help.search(&quot;geom_&quot;, package = &quot;ggplot2&quot;) Una vez añadida una capa al gráfico a este pueden agregarse nuevas capas ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point() ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point()+ geom_smooth() 6.1.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_smooth() + facet_grid(~ Species) crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 6.1.4 Más sobre estéticas Las estéticas se pueden etiquetar con la función labs. Además, se le puede añadir un título al gráfico usando la función ggtitle. Por ejemplo, en el gráfico anterior se pueden re etiquetar los ejes y la leyenda haciendo 6.2 Referencias What is Exploratory Data Analysis? What is EDA? Diagrama BoxPlot Ggplot2: Elegant Graphics for Data Analysis Plotly Clustering "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
